\section{Question 2:Parameter Selection and Classification (for dataset B)}
\subsection{Data preprocessing}
Z-score normalization was used for normalizing data instead of the min-max normalization. The Z-Score normalization is a standardization and re-scaling the range of the data in which the mean is centered on zero and it has a unit variance. This means that z-score will still preserve the range of the data (maximum and minimum) as min-max normalization but also will provide the standard deviation and variance of the distribution. This is helpful when it comes for dealing with data flowing in real time as we can easily normalize data coming in using the mean and variance, however, if we used min-max normalization then we might run into an issue where the data coming in is larger than the max the we trained model. This is an advantage that we get by choosing the z-score over the min-max. \\
We split the test and training set randomly in order to evaluate the performance of the models (Classifiers). \\
For this dataset, the distribution of the labels is almost the same. We have 1137 samples labeled as 1 and 1063 samples labeled as -1.


\\
\subsection{Parameter selection:}
\subsubsection{Parameter selection for k-NN}

A k-NN classifier was employed to predict the labels of the data in data set B based on the input of the 56 features. The main hyper parameter of the k-NN model is the number of neighbours, "k". A set of possible values for the hyper parameter were tested and validated using 5 fold cross validation. Each fold represents a slice of the data and one fold at a time is used as the validation set to verify the performance of the model. This model was not validated directly on the test set because 5 fold cross validation intentionally using one of the folds as a test set. In the case of 5 fold the resulting test train split is effectively 80/20. The test set can then be used to compared different models with equal fairness knowing that none of the data would bleed through during the cross validation step.

For each value of K the cross validated accuracy score was returned as the key performance metric. Figure~\ref{fig:knntuning} shows the plot of the accuracy vs the value  of the parameter "k". We see that the model performed best when the value of "k" was set to 15.

\clearpage{}
\begin{figure}[!ht]
 \centering
\includegraphics[width=6.1in]{assignment2/2-2-a-kNN.png}
\caption{\label{fig:knntuning} Relationship between accuracy and the parameter k }
\end{figure}



\subsubsection{Parameter selection for SVM}

A support vector machine model was created to predict labels of the given data set. There were two hyper parameters that need to be determined for the SVM model, "C" and "gamma". In order to determine the optimal values a grid search was done over the parameter space with the given set of values. The scoring metric used for each combination of parameters was "roc_auc". The result of the hyper parameter tuning was a value of 0.01 for gamma and 10 for C.

The model was then trained on the training data using the hyper parameters found during the cross validation grid search. Figure~\ref{fig:svmtuning} shows the plot of AUCROC for the tuned and trained SVM model. The AUC of the model is 0.97.


\begin{figure}[!ht]
 \centering
\includegraphics[width=6.1in]{assignment2/2-2-b-svm.png}
\caption{\label{fig:svmtuning} SVM area under curve receiver operator characteristic}
\end{figure}


\subsection{Training six classifiers}

\subsubsection{Classify the test set using k-NN, SVM, Random Forests and Neural Networks. Use the chosen parameters from the parameter selection process in question 2 for k-NN and SVM. For the next two classifiers use the default setups listed at the end for Random Forests and Neural Networks}


The performance for the four models (KNN, SVM, Random Forest and Neural Network) was calculated using the sklearn library function "classification\_report". We can see the results of the classification in Figure~\ref{fig:resultsknnsvm} and Figure~\ref{fig:resulstsrfnn} below. From the results we can see that the KNN model performed the worst with an average F1 score of 0.75. The SVM model and Neural Network with default parameters performed similarly with and average f1 score of 0.91 and 0.90 respectively. Finally the random forest model out performed the other models with an average f1 score of 0.95.

It is important to mention that there are multiple metrics that can be used to evaluate the performance of a model on a binary classification problem. Depending on the data provided and the use case of the model some outcomes may be more desired that others. For example the true positive rate may be more important for a bank to determine default likelihood because of the impact to their business if a false negative is provided. Knowing this it may be more important to value recall over precision or vice-versa. The f1-score does a decent job of blending the performance of both scenarios and is therefore used in this report as the general metric to evaluate performance between these models.

Overall there is not much variation between the various scoring metrics. This is most likely due to the fact that the data set provided is fairly balanced. The count of the true labels and false labels is quite similar. If the data was imbalanced techniques could have been used to reduce the impact to the models where labels are biased to the majority class. This problem may have been an issue for the classifiers found in part 1 as there was a large imbalance between the majority and minority classes.

% \clearpage{}
\begin{figure}[!ht]
 \centering
\includegraphics[height=3.5in]{assignment2/2-3-a1.png}
\caption{\label{fig:resultsknnsvm} Performance Result for KNN and SVM models using parameters from section 2.2}
\end{figure}
\clearpage{}
\begin{figure}[!ht]
 \centering
\includegraphics[height=3.5in]{assignment2/2-3-a2.png}
\caption{\label{fig:resulstsrfnn} Performance Result for Random Forest and Neural Network models using default parameters}
\end{figure}


\subsubsection{For the fifth and sixth classifiers, you should explore the parameters of the Random Forests and Neural Network models to devise your own classifier instance that does better than the other methods. For example, you could consider a deeper neural network with multiple layers, use different optimization/solver algorithms, you could modify the Random Forests using different parameter settings for depth and number of trees or enable boosting. Play around with options and choose a setting for RFs and NNs that performs better}

Our first goal is to increase the performance of the random forest model in predicting the test subset of the provided data set. Our approach is to use linear search over two important hyper parameters of the random forest model, "n\_estimators" and "treedepth". Figure~\ref{fig:rfnest} below shows the plot of AUC for both the train and test data. We can see from the plot that the performance of the test AUC starts to decrease when n\_estimators reaches a value greater than 100. Based on this result 100 was chosen for the hyper parameter. 


\begin{figure}[!ht]
 \centering
\includegraphics[width=6.1in]{assignment2/2-3-b-rf(n_estimator).png}
\caption{\label{fig:rfnest} Random Forest linear search for optimal n\_estimator parameter value}
\end{figure}

\clearpage{}
Next, a linear search was done to find the optimal value of tree depth. Figure~\ref{fig:rfnest} below shows the plot of AUC vs tree depth. We can see that the test and train data begin to diverge around a tree depth of 5, however test accuracy increases slightly until tree dept is 13. We decided to use a tree depth of 13 for the hyper parameter value.

\begin{figure}[!ht]
 \centering
\includegraphics[width=6.1in]{assignment2/2-3-b-rf(Treedepth).png}
\caption{\label{fig:rftreedepth} Random Forest linear search for optimal treedepth parameter value}
\end{figure}

The result of the before and after for the random forest hyper parameter tuning can be seen in Figure~\ref{fig:resulstsrftune}. There was a fairly decent performance improvement from 0.95 f1 score to 0.97. Other methods to further improve performance could be explored such as grid search over a larger parameter space or boosting the tree by applying penalties to the cost function if complexity of the tree increases (also called regularization term).

\begin{figure}[!ht]
 \centering
\includegraphics[height=3.5in]{assignment2/2-3-b-rfresult.png}
\caption{\label{fig:resulstsrftune} Performance Result for Random Forest before and after tuning hyper parameters}
\end{figure}

\clearpage{}
A similar process was done for the neural network model. We used a grid search to search over multiple parameters for the neural network model, this includes: hidden layer architecture, activation, solver, alpha, and batch size. We evaluated 4 architectures when training the neural network: (57,57,57), (57,57),(57,30,10), and (57,57,30,20,5). After performing the grid search it was determined that the best performing architecture was a 2 hidden layer with 57 neurons. The remaining parameters were as follows: "'activation': 'relu', 'alpha': 0.05, 'batch\_size': 70, 'hidden\_layer\_sizes': (57, 57), 'max\_iter': 2000, 'random\_state': 42, 'solver': 'adam'"

Figure~\ref{fig:resulstsnntune} below shows the result of the hyper parameter tuning. We can see that the neural network performance increased slightly from 0.90 to 0.92 f1 score. Due to limitation with sklearn library it was difficult to plot the training error and test error vs epoch using the MLPclassifier class. In the future keras will be used to better determine training metrics and characteristics when training. 

\begin{figure}[!ht]
 \centering
\includegraphics[height=3.5in]{assignment2/2-3-b-nnresult.png}
\caption{\label{fig:resulstsnntune} Performance Result for Neural Network before and after tuning hyper parameters}
\end{figure}




\subsubsection{Repeat each classification method 20 times by varying the split of the training-test set as in question 2-2. Report the average and standard deviation of classification performance on the test set regarding: accuracy, precision, recall, and F- Measure. Also report the training time and classification time of all the methods. Explain why the classification was repeated 20 times}

Each classification method was repeated 20 times for each value of k in \{4,5,6,7\}. The value K represented the fold in the cross validation. After completion the mean and standard deviation is calculated for each performance metric for every fold. Figures~\ref{fig:run1} to \ref{fig:run6} below show each table of values for each of the 6 classification methods.


\begin{figure}[!ht]
 \centering
\includegraphics[width=\textwidth]{assignment2/2-4-run1.png}
\caption{\label{fig:run1} KNN performance metrics over 20 iterations of cross validation for different k folds}
\end{figure}

\begin{figure}[!ht]
 \centering
\includegraphics[width=\textwidth]{assignment2/2-4-run2.png}
\caption{\label{fig:run2} SVM performance metrics over 20 iterations of cross validation for different k folds}
\end{figure}

\begin{figure}[!ht]
 \centering
\includegraphics[width=\textwidth]{assignment2/2-4-run3.png}
\caption{\label{fig:run3} Random Forest with default parameters performance metrics over 20 iterations of cross validation for different k folds}
\end{figure}

\begin{figure}[!ht]
 \centering
\includegraphics[width=\textwidth]{assignment2/2-4-run4.png}
\caption{\label{fig:run4} Neural Network with default parameters performance metrics over 20 iterations of cross validation for different k folds}
\end{figure}

\begin{figure}[!ht]
 \centering
\includegraphics[width=\textwidth]{assignment2/2-4-run5.png}
\caption{\label{fig:run5} Random Forest with tuned parameters performance metrics over 20 iterations of cross validation for different k folds}
\end{figure}

\begin{figure}[!ht]
 \centering
\includegraphics[width=\textwidth]{assignment2/2-4-run6.png}
\caption{\label{fig:run6} Neural Network with tuned parameters performance metrics over 20 iterations of cross validation for different k folds}
\end{figure}




\subsection{Obtained Results}


Based on the results obtained above, we see that k-NN performed the worst overall in terms of accuracy, precision, recall and f measure, however, it was the fastest in terms of training the model. Random forest with the tuned parameters performed the best among all, however, slower compared to Neural networks in terms of classification time. 
Neural networks in the other hand is the fastest at classifying but the slowest in training the model. SVM provided a good metrics measurements compared to knn but it took much more time to train the model and to classify. The tuned parameters for random forests and neural network performed significantly compared to their default hyperparameters. From this assignmnet, we can conclude that the Neural network and random forest are by far the best models for this type of dataset when they are tuned. 



\subsection{Feature Removal}
We can remove the redundant feature from the dataset, Attributes that are not necessary decrease training speed,also tend to decrease our model's performance, most importantly,also generalization on the the test's performance is reduced.