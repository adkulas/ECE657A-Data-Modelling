\section{Question 2:Parameter Selection and Classification (for dataset B)}
\subsection{Data preprocessing}
Z-score normalization was used for normalizing data instead of the min-max normalization. The Z-Score normalization is a standardization and re-scaling the range of the data in which the mean is centered on zero and it has a unit variance. This means that z-score will still preserve the range of the data (maximum and minimum) as min-max normalization but also will provide the standard deviation and variance of the distribution. This is helpful when it comes for dealing with data flowing in real time as we can easily normalize data coming in using the mean and variance, however, if we used min-max normalization then we might run into an issue where the data coming in is larger than the max the we trained model. This is an advantage that we get by choosing the z-score over the min-max. \\
We split the test and training set randomly in order to evaluate the performance of the models (Classifiers). \\
For this dataset, the distribution of the labels is almost the same. We have 1137 samples labeled as 1 and 1063 samples labeled as -1.


\\
\subsection{Parameter selection:}
\subsubsection{Parameter selection for k-NN}

A k-NN classifier was employed to predict the labels of the data in data set B based on the input of the 56 features. The main hyper parameter of the k-NN model is the number of neighbours, "k". A set of possible values for the hyper parameter were tested and validated using 5 fold cross validation. Each fold represents a slice of the data and one fold at a time is used as the validation set to verify the performance of the model. This model was not validated directly on the test set because 5 fold cross validation intentionally using one of the folds as a test set. In the case of 5 fold the resulting test train split is effectively 80/20. The test set can then be used to compared different models with equal fairness knowing that none of the data would bleed through during the cross validation step.

For each value of K the cross validated accuracy score was returned as the key performance metric. Figure~\ref{fig:knntuning} shows the plot of the accuracy vs the value  of the parameter "k". We see that the model performed best when the value of "k" was set to 15.

\clearpage{}
\begin{figure}[!ht]
 \centering
\includegraphics[width=6.1in]{assignment2/2-2-a-kNN.png}
\caption{\label{fig:knntuning} Relationship between accuracy and the parameter k }
\end{figure}



\subsubsection{Parameter selection for SVM}

A support vector machine model was created to predict labels of the given data set. There were two hyper parameters that need to be determined for the SVM model, "C" and "gamma". In order to determine the optimal values a grid search was done over the parameter space with the given set of values. The scoring metric used for each combination of parameters was "roc_auc". The result of the hyper parameter tuning was a value of 0.01 for gamma and 10 for C.

The model was then trained on the training data using the hyper parameters found during the cross validation grid search. Figure~\ref{fig:svmtuning} shows the plot of AUCROC for the tuned and trained SVM model. The AUC of the model is 0.97.


\begin{figure}[!ht]
 \centering
\includegraphics[width=6.1in]{assignment2/2-2-b-svm.png}
\caption{\label{fig:svmtuning} SVM area under curve receiver operator characteristic}
\end{figure}


\subsection{Training six classifiers}

\subsubsection{Classify the test set using k-NN, SVM, Random Forests and Neural Networks. Use the chosen parameters from the parameter selection process in question 2 for k-NN and SVM. For the next two classifiers use the default setups listed at the end for Random Forests and Neural Networks}


\subsubsection{For the fifth and sixth classifiers, you should explore the parameters of the Random Forests and Neural Network models to devise your own classifier instance that does better than the other methods. For example, you could consider a deeper neural network with multiple layers, use different optimization/solver algorithms, you could modify the Random Forests using different parameter settings for depth and number of trees or enable boosting. Play around with options and choose a setting for RFs and NNs that performs better}


\subsubsection{Repeat each classification method 20 times by varying the split of the training-test set as in question 2-2. Report the average and standard deviation of classification performance on the test set regarding: accuracy, precision, recall, and F- Measure. Also report the training time and classification time of all the methods. Explain why the classification was repeated 20 times}



\subsection{Obtained Results}



\subsection{Feature Removal}
