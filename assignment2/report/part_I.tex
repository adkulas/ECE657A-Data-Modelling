\section{Question I: Revisiting HW4 Bank Classification with New Tools (for dataset A)}
\subsection{Data preprocessing}
For this part of the assignment, the dataset had to be analyzed first as part of the preprocessing step to get the structure ,detect issues such as missing values and outliers. The bank marketing data(bank-additional.csv) has 4119 observations and 21 features.There was found to be 1230 unknowns in the categorical features of the dataset. Features that had unknowns were checked and there was found to be six features: job,marital,education,default,housing,loan that had unknowns( 0.96 ,0.3 ,4.1,  19.5, 2.6)\% respectively.Removing rows with missing values can be too limiting on some predictive modeling problems. The proportion of the unknown's is sparse, the mode of the categorical features was used to replace the unknown's. Also, the "duration" feature was dropped since this attribute highly affects the output target (e.g., if duration=0 then y='no').
The categorical predictor variables  were converted  to numerical using get\_dummies ,appending the structure to 47 features (4119,47).
Outliers were found by first finding the standard deviation of each feature and then using each standard deviation to find values in that specific feature that vary by more than 3 standard deviations. 

\subsection{Dividing data into training and testing}
For dividing the data-set(4119 samples), 80\% was used for training and 20\% for testing. The reason for this split is because of the data imbalance as we have  3,668 samples labeled as no as compared to 451 labeled as yes. By using 80\% of the data for training the model, we are providing more information for the model to learn about the yes which improves the prediction for this label. %The 80-20 split ensures all cases is accounted for in the training phase and our algorithm is generalizing and not memorizing. 


\subsection{Applying classification}
%Figure~\ref{fig:fig1} shows the comparison between histogram plots of feature 9 and 24 before and after normalization. 

Decision Tree (DT):\\ The Depth of the tree was chosen as 3 for which the model performed more accurately without over-fitting. We first plotted the area under curve score (AUC score) against the tree depth (figure 1 below).
\begin{figure}[!ht]
 \centering
\includegraphics[width=6.1in]{assignment2/1-3-DecisionTree_AUC.png}
\caption{\label{fig:fig1}AUC score curve against tree depth}
\end{figure}
As it can be seen from the curve, the deeper the tree the higher the train AUC score but the test AUC score wont change a lot after tree depth of 3. This means that we get an overfitting case as our model will have a high AUC score (predicts the train data perfectly) for deeper tree depth but will fail to generalize and predict the test data. For this reason the tree depth was chose to be 3 for our model.
If the depth of tree was not specified, by default, scikit-learn will keep expanding the nodes until all the leaves contain less than min\_samples\_split samples and as we saw in the graph below, the higher value of maximum depth causes over-fitting, and a lower value causes under-fitting\cite{ref_url1}.


ID3, or Iternative Dichotomizer, was the first of three Decision Tree implementations developed by Ross Quinlan
It builds a decision tree for the given data in a top-down fashion, starting from a set of objects and a specification of properties Resources and Information. each node of the tree, one property is tested based on maximizing information gain and minimizing entropy, and the results are used to split the object set. This process is recursively done until the set in a given sub-tree is homogeneous (i.e. it contains objects belonging to the same category). The ID3 algorithm uses a greedy search. It selects a test using the information gain criterion, and then never explores the possibility of alternate choices.

CART stands for Classification and Regression Trees. It is characterized by the fact that it constructs binary trees, namely each internal node has exactly two outgoing edges. The splits are selected using the twoing criteria and the obtained tree is pruned by costâ€“complexity Pruning. CART can handle both numeric and categorical variables and it can easily handle outliers.
\\

Random Forests (RF): \\
The random forests classifier was built using the scikit-learn. The two main parameters that were considered here are the n\_estimator and tree depth. The n\_estimator simply represnts the number of trees in the model and tree depth is the depth of those tree. Similar to the approach described in the decision tree, AUC score was plotted against n\_estimator and tree depth to avoid any over-fitting cases. Figure~\ref{fig:fig2} shows the AUC scores for n\_estimator and figure~\ref{fig:fig3} shows the AUC scores for tree depth. 
\begin{figure}[!ht]
 \centering
\includegraphics[width=6.1in]{assignment2/1-3-RandomForests_AUC(n_estimators).png}
\caption{\label{fig:fig2}AUC score curve against n\_estimators}
\end{figure}

\begin{figure}[!ht]
 \centering
\includegraphics[width=6.1in]{assignment2/1-3-RandomForests_AUC(TreeDepth).png}
\caption{\label{fig:fig3}AUC score curve against tree depth}
\end{figure}

\\

Neural Network (NN): \\
Neural netwrok is made up of three main layers: input, hidden, and output layers. In our case, the input layer is made up of 46 layers (number of input features) and the output layer has a single node (outputs either 1 or 0). The way the neural network model works is that it receives an inputs and multiply them by some weight which is then passed to an activation function to predict an output. Then the output is compared with y\_test label in which the weights will be adjusted accordingly. The process repeated until the maximum number of iterations reached. For building our model, we used gridsearch built-in function which returns the best hyper-parameters from the parameter space list provided. The hyper-parameters that were tested are:\\
1- hidden layer sizes : [(32,32), (20,20), (47,47)] \\
2- activation function: ['tanh', 'relu'] \\
3- solver: ['sgd', 'adam','lbfgs'] \\
4- alpha: [0.0001, 0.05] \\
then the gridsearch returned the parameters that best performed. However, one thing to note is that even though there is no rule of thumb for choosing the number of neurons in the hidden layers, it should not exceed the number of input layers (46 in this case) to avoid over-fitting problems. 
The best parameters were found to be as follow: activation=tanh, alpha=0.0001, hidden layers = (32, 32), and solver = Stochastic Gradient Descent (SGD). 

\\
\subsection{Create a few plots of your model on the test data, two of the data dimensions at a time,indicating the predicted elements of each class using different colors or shapes. You may need to try plotting various pairs of dimensions to see which provide some interesting result. Be sure to label your axis and legend. Why is separation better on some plots than others}

\\



\subsection{Produce a table with the true/false positive/negative metrics as well as accuracy's. Compare the values using bar charts}

Figure~\ref{fig:fig4} shows the comparison between true/false positive/negative metrics as well as accuracies
\begin{figure}[!ht]
 \centering
\includegraphics[width=6.1in]{assignment2/1-5_table.png}
\caption{\label{fig:fig4}table of metrics/accuracies and algorithms}
\end{figure}

Figure~\ref{fig:fig5} shows the comparison between true/false positive/negative metrics as well as accuracies
\begin{figure}[!ht]
 \centering
\includegraphics[width=6.1in]{assignment2/80-20_barcharts_algorithms.png}
\caption{\label{fig:fig5}bar charts of metrics/accuracies and algorithms}
\end{figure}

\\

\subsection{Provide a short explanation of the results you have shown and what it means. Which classification method performed better? Why? Contrast performance with classification from the previous homework and comment on the difference, if any}


\\

\subsection{ Fun/Bonus: attempt at least one method to tackle the discrepancy in the size of the classes (imbalanced data)}
There are several methods to handle imbalanced data (https://elitedatascience.com/imbalanced-classes)
1 Up-sample Minority Class(attempted)
2 Down-sample Majority Class(attempted)
3 Change Your Performance Metric(attempted)
4 Penalize Algorithms (Cost-Sensitive Training)
5 Use Tree-Based Algorithms
Change Your Performance Metric
 I think to tackle the issue of imbalanced data,precision  recall curve should be used when analyzing each algorithms performance  .As we know , ROC/AUC uses TPR(True positive rate) on Y axis and FPR(False positive rate ) on X axis. So replacing the FPR with precision on the X axis of the AUC graph might be the way to go.
 Precision=(TP/TP +FP) FPR= (FP/TN+FP)
 Since there were lots of samples that were Negatives(No's) relative to the number of Positives(Yes's) samples, then precision might be more useful than false positive rate . This is because precision does not include the number of True Negatives in its calculation, and is not affected by imbalance.



