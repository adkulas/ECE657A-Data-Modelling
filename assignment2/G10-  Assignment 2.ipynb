{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn import neighbors, linear_model\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score, roc_curve, auc\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "import pydot\n",
    "import os\n",
    "\n",
    "from mlxtend.plotting import plot_learning_curves\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question I: Revisiting HW4 Bank Classi\f",
    "cation with New Tools (for dataset A) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Loading the dataset and performing data preprocessing\n",
    "\n",
    "> Load a simple dataset and perform some basic data preprocessing to fill out ”unknowns”,\n",
    "outliers or other invalid data. Explain what preprocessing was performed and why. Also,\n",
    "change categorical data into numerical features using pandas.get dummies [5]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_addidtional_full = \"bank-additional.csv\"\n",
    "\n",
    "data = pd.read_csv(bank_addidtional_full, sep = ';', na_values=[\"unknown\"])\n",
    "df = pd.DataFrame(data)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We first check for missing values in the dataset. There was 1230 entries as unknown in the catogerical features that were replaced with NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())\n",
    "print(df.shape)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NaN values were replaced by the mode of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.fillna(df.mode().iloc[0])\n",
    "df.drop(['default'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then we convert all the catogrical columns to numerical columns. Also, dropping the duration column since is a predictive variable according to UCI website and should be dropped if the purpose is to build a realistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df._get_numeric_data().head()\n",
    "df['y'] = df['y'].map({'yes': 1, 'no': 0})\n",
    "df.drop(['duration'], axis=1, inplace=True)\n",
    "df = pd.get_dummies(df,drop_first=True)\n",
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(df.columns.values) #Make a list of all of the columns in the df\n",
    "cols.pop(cols.index('y')) #Remove y from list\n",
    "df = df[cols + ['y']] #Create new dataframe with columns in the order you want\n",
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.boxplot(x=df['nr.employed'])\n",
    "# df['cons.conf.idx'].value_counts()\n",
    "# df['cons.conf.idx'].describe()\n",
    "# data['pdays'].isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "plt.figure(figsize=(20, 20))\n",
    "# plot True positives\n",
    "\n",
    "# columns = ('campaign', 'previous', 'emp.var.rate','cons.price.idx','cons.conf.idx',\n",
    "#           'euribor3m', 'nr.employed')\n",
    "# x_axis = np.arange(len(algorithms))\n",
    "# dt_trueP = confusion_matrix(y_test,y_pred_dt)[1][1]\n",
    "# rf_trueP = confusion_matrix(y_test,RFC_pred)[1][1]\n",
    "# nn_trueP = confusion_matrix(y_true,y_pred)[1][1]\n",
    "# trueP = [dt_trueP,rf_trueP,nn_trueP]\n",
    " \n",
    "\n",
    "plt.subplot(4, 2, 1) \n",
    "sns.boxplot(x=df['campaign'])\n",
    "\n",
    "plt.subplot(4, 2, 2)\n",
    "sns.boxplot(x=df['previous'])\n",
    "\n",
    "plt.subplot(4, 2, 3)\n",
    "sns.boxplot(x=df['emp.var.rate'])\n",
    "\n",
    "plt.subplot(4, 2, 4)\n",
    "sns.boxplot(x=df['cons.price.idx'])\n",
    "\n",
    "plt.subplot(4, 2, 5)\n",
    "sns.boxplot(x=df['cons.conf.idx'])\n",
    "\n",
    "plt.subplot(4, 2, 6)\n",
    "sns.boxplot(x=df['euribor3m'])\n",
    "\n",
    "plt.subplot(4, 2, 7)\n",
    "sns.boxplot(x=df['nr.employed'])\n",
    "\n",
    "     \n",
    "plt.savefig(os.path.join('barcharts_algorithms.png'), dpi=300, format='png', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.pdays = df.pdays.replace({999: np.nan })\n",
    "# display(df.pdays.describe())\n",
    "# sns.boxplot(x=df['pdays'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.pdays = df.pdays.fillna(df.pdays.mean() + df.pdays.std()*6)\n",
    "# sns.boxplot(x=df['pdays'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now detecting the outliers and keeping samples that are only within 3 standard deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[(np.abs(stats.zscore(df)) < 3).all(axis=1)]\n",
    "# keep only the ones that are within +3 to -3 standard deviations in the column 'Data'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['y'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Dividing data into training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into train and test data. The test data size chosen to be 30%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('y',axis=1)\n",
    "y = df['y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.20,random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Applying classification: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a) Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, the classifier was instantiated with the default parameters and then was tuned later for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier(random_state=101)\n",
    "dtree.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = dtree.predict(X_test)\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print('\\n')\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC scores is plotted below to check for overfitting cases with depth of the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depths = np.linspace(1, 32, 32, endpoint=True)\n",
    "train_results = []\n",
    "test_results = []\n",
    "\n",
    "for max_depth in max_depths:\n",
    "   dt = DecisionTreeClassifier(max_depth=max_depth)\n",
    "   dt.fit(X_train, y_train)\n",
    "   train_pred = dt.predict(X_train)\n",
    "   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n",
    "   roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "   # Add auc score to previous train results\n",
    "   train_results.append(roc_auc)\n",
    "   y_pred = dt.predict(X_test)\n",
    "   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "   roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "   # Add auc score to previous test results\n",
    "   test_results.append(roc_auc)\n",
    "    \n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "line1, = plt.plot(max_depths, train_results, 'b', label=\"Train AUC\")\n",
    "line2, = plt.plot(max_depths, test_results, 'r', label=\"Test AUC\")\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.ylabel('AUC score')\n",
    "plt.xlabel('Tree depth')\n",
    "plt.title('AUC plot for Decicion Tree')\n",
    "plt.savefig(os.path.join('1-3-DecisionTree_AUC.png'), dpi=300, format='png', bbox_inches='tight')\n",
    "plt.show()\n",
    "#Source: https://medium.com/all-things-ai/in-depth-parameter-tuning-for-random-forest-d67bb7e920d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the graph above, we see that when we have a high tree depth we get an overfitting case where the model can predicts the train data perfectly (high area under curve), however, the models fails to generalize and predict new data (test data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gini = DecisionTreeClassifier(criterion = \"gini\", random_state = 100, max_depth=3, min_samples_leaf=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf = clf_gini.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dt = dt_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy is \", accuracy_score(y_test,y_pred_dt)*100)\n",
    "print(confusion_matrix(y_test,y_pred_dt))\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_dt).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "print('\\n')\n",
    "print(classification_report(y_test,y_pred_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples_splits = np.linspace(0.1, 1.0, 10, endpoint=True)\n",
    "train_results = []\n",
    "test_results = []\n",
    "for min_samples_split in min_samples_splits:\n",
    "   dt = DecisionTreeClassifier(min_samples_split=min_samples_split)\n",
    "   dt.fit(X_train, y_train)\n",
    "   train_pred = dt.predict(X_train)\n",
    "   false_positive_rate, true_positive_rate, thresholds =    roc_curve(y_train, train_pred)\n",
    "   roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "   train_results.append(roc_auc)\n",
    "   y_pred = dt.predict(X_test)\n",
    "   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "   roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "   test_results.append(roc_auc)\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "line1, = plt.plot(min_samples_splits, train_results, 'b', label=\"Train AUC\")\n",
    "line2, = plt.plot(min_samples_splits, test_results, 'r', label=\"Test AUC\")\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.ylabel('AUC score')\n",
    "plt.xlabel('min samples split')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_samples_leafs = np.linspace(0.1, 0.5, 5, endpoint=True)\n",
    "# train_results = []\n",
    "# test_results = []\n",
    "# for min_samples_leaf in min_samples_leafs:\n",
    "#    dt = DecisionTreeClassifier(min_samples_leaf=min_samples_leaf)\n",
    "#    dt.fit(X_train, y_train)\n",
    "#    train_pred = dt.predict(X_train)\n",
    "#    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n",
    "#    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "#    train_results.append(roc_auc)\n",
    "#    y_pred = dt.predict(X_test)\n",
    "#    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "#    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "#    test_results.append(roc_auc)\n",
    "# from matplotlib.legend_handler import HandlerLine2D\n",
    "# line1, = plt.plot(min_samples_leafs, train_results, 'b', label=\"Train AUC\")\n",
    "# line2, = plt.plot(min_samples_leafs, test_results, 'r', label=\"Test AUC\")\n",
    "# plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "# plt.ylabel('AUC score')\n",
    "# plt.xlabel('min samples leaf')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Visualization using the the built-in visualization from Scikit learn. This requires to install the pydot library and Graphviz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image  \n",
    "from sklearn.externals.six import StringIO  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot \n",
    "\n",
    "features = list(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = StringIO()  \n",
    "export_graphviz(clf_gini, out_file=dot_data,feature_names=features,filled=True,rounded=True)\n",
    "\n",
    "graph = pydot.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph[0].create_png()) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b) Random Forests (RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first fit the model with default parameters and evaluate the performance, then we will tune the parameters and compare the performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state=101)\n",
    "rfc_clf = rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_pred = rfc_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy is \", accuracy_score(y_test,rfc_pred)*100)\n",
    "print(confusion_matrix(y_test,rfc_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_test,rfc_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to DT classifier, we will use the area under curve AUC to evaluate the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [1, 2, 4, 8, 16, 32, 64, 100, 200]\n",
    "train_results = []\n",
    "test_results = []\n",
    "\n",
    "for estimator in n_estimators:\n",
    "   rf = RandomForestClassifier(n_estimators=estimator, n_jobs=-1, random_state=101)\n",
    "   rf.fit(X_train, y_train)\n",
    "   train_pred = rf.predict(X_train)\n",
    "   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n",
    "   roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "   train_results.append(roc_auc)\n",
    "   y_pred = rf.predict(X_test)\n",
    "   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "   roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "   test_results.append(roc_auc)\n",
    "\n",
    "    \n",
    "line1, = plt.plot(n_estimators, train_results, 'b', label=\"Train AUC\")\n",
    "line2, = plt.plot(n_estimators, test_results, 'r', label=\"Test AUC\")\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.ylabel('AUC score')\n",
    "plt.xlabel('n_estimators')\n",
    "plt.title('AUC plot for Random Forests')\n",
    "plt.savefig(os.path.join('1-3-RandomForests_AUC(n_estimators).png'), dpi=300, format='png', bbox_inches='tight')\n",
    "plt.show()\n",
    "# Source: https://medium.com/all-things-ai/in-depth-parameter-tuning-for-random-forest-d67bb7e920d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The n_estimators in the random forests classifier represents the numsber of trees used in the classifier. Based on the AUC graph above, we see that the highest AUC score for the test data will be around 16 n_estimators. Increasing the n_estimators decreases the test perfomance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depths = np.linspace(1, 32, 32, endpoint=True)\n",
    "train_results = []\n",
    "test_results = []\n",
    "\n",
    "for max_depth in max_depths:\n",
    "   rf = RandomForestClassifier(max_depth=max_depth, n_jobs=-1, random_state=101)\n",
    "   rf.fit(X_train, y_train)\n",
    "   train_pred = rf.predict(X_train)\n",
    "   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n",
    "   roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "   train_results.append(roc_auc)\n",
    "   y_pred = rf.predict(X_test)\n",
    "   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "   roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "   test_results.append(roc_auc)\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "line1, = plt.plot(max_depths, train_results, 'b', label=\"Train AUC\")\n",
    "line2, = plt.plot(max_depths, test_results, 'r', label=\"Test AUC\")\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.ylabel('AUC score')\n",
    "plt.xlabel('Tree depth')\n",
    "plt.title('AUC plot for Random Forests')\n",
    "plt.savefig(os.path.join('1-3-RandomForests_AUC(TreeDepth).png'), dpi=300, format='png', bbox_inches='tight')\n",
    "plt.show()\n",
    "#Source: https://medium.com/all-things-ai/in-depth-parameter-tuning-for-random-forest-d67bb7e920d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the higher the tree depth, we get an overfitting case. So a tree depth of 3 wil be chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFC = RandomForestClassifier(n_estimators=16,random_state=101, max_depth=3)\n",
    "RFC_clf = RFC.fit(X_train, y_train)\n",
    "RFC_pred = RFC.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"Results from the default parameters:\")\n",
    "print(\"Accuracy is \", accuracy_score(y_test,rfc_pred)*100)\n",
    "print(confusion_matrix(y_test,rfc_pred))\n",
    "print(classification_report(y_test,rfc_pred))\n",
    "print('\\n')\n",
    "print(\"Results from the tunned parameters:\")\n",
    "print(\"Accuracy is \", accuracy_score(y_test,RFC_pred)*100)\n",
    "print(confusion_matrix(y_test,RFC_pred))\n",
    "print(classification_report(y_test,RFC_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C) Neural Networks (NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data should be normalized before training the neural network model. This is because the NN model might not converge before the max number of iterations allowed. Also, the multi-layer perception is sensitive to scaling the features. (source: https://www.kdnuggets.com/2016/10/beginners-guide-neural-networks-python-scikit-learn.html/2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(df.drop('y',axis=1))\n",
    "scaled_features = scaler.transform(df.drop('y',axis=1))\n",
    "\n",
    "df_feat = pd.DataFrame(scaled_features,columns=df.columns[:-1])\n",
    "\n",
    "X_nn = df_feat.loc[:,]\n",
    "y_nn = df['y']\n",
    "X_train_nn, X_test_nn, y_train_nn, y_test_nn = train_test_split(X_nn,y_nn, test_size=0.20,random_state=101)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_nn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with 2 layers with number of neurons:(36,36)\n",
    "mlp = MLPClassifier(max_iter=1000,random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_clf = mlp.fit(X_train_nn,y_train_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predicting and evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nn_clf.predict(X_test_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy is \", accuracy_score(y_test,predictions)*100)\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(\"\\n\")\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tunning the NN parameters using the GridSearchCV method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_space = {\n",
    "    'max_iter': [2000],\n",
    "    'hidden_layer_sizes': [(32,32), (20,20), (47,47)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam','lbfgs'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant'],\n",
    "    'random_state': [101]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3, refit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train_nn, y_train_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameter set\n",
    "print('Best parameters found:\\n', clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_nn = MLPClassifier(activation = 'tanh',alpha = 0.0001, learning_rate= 'constant', solver = 'sgd', random_state=101, hidden_layer_sizes=(20,20), max_iter=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_nn.fit(X_train_nn, y_train_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = y_test_nn , clf.predict(X_test_nn)\n",
    "\n",
    "print(\"Results from the default parameters:\")\n",
    "print(\"Accuracy is \", accuracy_score(y_test,predictions)*100)\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))\n",
    "print(\"\\n\")\n",
    "print(\"Results from the tunned parameters:\")\n",
    "print(\"Accuracy is \", accuracy_score(y_true,y_pred)*100)\n",
    "print(confusion_matrix(y_true,y_pred))\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Creating plots of the models on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting best two features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalized Data for Nural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_testnn_df =pd.DataFrame(X_test_nn, columns=df.columns[:-1]) \n",
    "y_testnn_df = pd.DataFrame(y_test_nn, columns = ['y'])\n",
    "X_testnn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_df =pd.DataFrame(X_test, columns=df.columns[:-1]) \n",
    "y_test_df = pd.DataFrame(y_test, columns = ['y'])\n",
    "X_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(X_test_df.columns, dt_clf.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_test_df.iloc[:, ]\n",
    "X = X[['pdays','nr.employed']]\n",
    "y= y_test_df['y']\n",
    "\n",
    "\n",
    "\n",
    "label = ['Decision Tree', 'Random Forests']\n",
    "clf_list = [clf_gini, RFC]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "grid = itertools.product([0,1],repeat=2)\n",
    "\n",
    "X = X.values\n",
    "y = y.values\n",
    "\n",
    "for clf, label, grd in zip(clf_list, label, grid):        \n",
    "    scores = cross_val_score(clf, X, y, cv=3, scoring='accuracy')\n",
    "    print(\"Accuracy: %.2f (+/- %.2f) [%s]\" %(scores.mean(), scores.std(), label))\n",
    "        \n",
    "    clf.fit(X, y)\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n",
    "    plt.title(label)\n",
    "    plt.xlabel('pdays')\n",
    "    plt.ylabel('nr.employed')\n",
    "    plt.savefig(os.path.join('1-4-dT_RF_(pdaysVsnr.employed).png'), dpi=300, format='png', bbox_inches='tight')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_testnn_df.iloc[:, ]\n",
    "X = X[['pdays','nr.employed']]\n",
    "y= y_testnn_df['y']\n",
    "\n",
    "\n",
    "label = ['Neural Network']\n",
    "clf_list = [mlp_nn]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "grid = itertools.product([0,1],repeat=2)\n",
    "\n",
    "X = X.values\n",
    "y = y.values\n",
    "\n",
    "\n",
    "for clf, label, grd in zip(clf_list, label, grid):        \n",
    "    scores = cross_val_score(clf, X, y, cv=3, scoring='accuracy')\n",
    "    print(\"Accuracy: %.2f (+/- %.2f) [%s]\" %(scores.mean(), scores.std(), label))\n",
    "        \n",
    "    clf.fit(X, y)\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n",
    "    plt.title(label)\n",
    "    plt.xlabel('pdays')\n",
    "    plt.ylabel('nr.employed')\n",
    "    plt.savefig(os.path.join('1-4-NN_(pdaysVsnr.employed).png'), dpi=300, format='png', bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_test_df.iloc[:, ]\n",
    "X = X[['nr.employed','euribor3m']]\n",
    "y= y_test_df['y']\n",
    "\n",
    "\n",
    "\n",
    "label = ['Decision Tree', 'Random Forests']\n",
    "clf_list = [clf_gini, RFC]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "grid = itertools.product([0,1],repeat=2)\n",
    "\n",
    "X = X.values\n",
    "y = y.values\n",
    "\n",
    "for clf, label, grd in zip(clf_list, label, grid):        \n",
    "    scores = cross_val_score(clf, X, y, cv=3, scoring='accuracy')\n",
    "    print(\"Accuracy: %.2f (+/- %.2f) [%s]\" %(scores.mean(), scores.std(), label))\n",
    "        \n",
    "    clf.fit(X, y)\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n",
    "    plt.title(label)\n",
    "    plt.xlabel('nr.employed')\n",
    "    plt.ylabel('euribor3m')\n",
    "    plt.savefig(os.path.join('1-4-dT_RF_(euribor3mVSnr.employed).png'), dpi=300, format='png', bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_testnn_df.iloc[:, ]\n",
    "X = X[['nr.employed','euribor3m']]\n",
    "y= y_testnn_df['y']\n",
    "\n",
    "\n",
    "label = ['Neural Network']\n",
    "clf_list = [mlp_nn]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "grid = itertools.product([0,1],repeat=2)\n",
    "\n",
    "X = X.values\n",
    "y = y.values\n",
    "\n",
    "\n",
    "for clf, label, grd in zip(clf_list, label, grid):        \n",
    "    scores = cross_val_score(clf, X, y, cv=3, scoring='accuracy')\n",
    "    print(\"Accuracy: %.2f (+/- %.2f) [%s]\" %(scores.mean(), scores.std(), label))\n",
    "        \n",
    "    clf.fit(X, y)\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n",
    "    plt.title(label)\n",
    "    plt.xlabel('nr.employed')\n",
    "    plt.ylabel('euribor3m')\n",
    "    plt.savefig(os.path.join('1-4-NN_(euribor3mVSnr.employed).png'), dpi=300, format='png', bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_test_df.iloc[:, ]\n",
    "X = X[['pdays','euribor3m']]\n",
    "y= y_test_df['y']\n",
    "\n",
    "\n",
    "label = ['Decision Tree', 'Random Forests']\n",
    "clf_list = [clf_gini, RFC]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "grid = itertools.product([0,1],repeat=2)\n",
    "\n",
    "X = X.values\n",
    "y = y.values\n",
    "\n",
    "\n",
    "for clf, label, grd in zip(clf_list, label, grid):        \n",
    "    scores = cross_val_score(clf, X, y, cv=3, scoring='accuracy')\n",
    "    print(\"Accuracy: %.2f (+/- %.2f) [%s]\" %(scores.mean(), scores.std(), label))\n",
    "        \n",
    "    clf.fit(X, y)\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n",
    "    plt.title(label)\n",
    "    plt.xlabel('pdays')\n",
    "    plt.ylabel('euribor3m')\n",
    "    plt.savefig(os.path.join('1-4-dT_RF_(pdaysVSeuribor3m).png'), dpi=300, format='png', bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_testnn_df.iloc[:, ]\n",
    "X = X[['pdays','euribor3m']]\n",
    "y= y_testnn_df['y']\n",
    "\n",
    "\n",
    "label = ['Neural Network']\n",
    "clf_list = [mlp_nn]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "grid = itertools.product([0,1],repeat=2)\n",
    "\n",
    "X = X.values\n",
    "y = y.values\n",
    "\n",
    "\n",
    "for clf, label, grd in zip(clf_list, label, grid):        \n",
    "    scores = cross_val_score(clf, X, y, cv=3, scoring='accuracy')\n",
    "    print(\"Accuracy: %.2f (+/- %.2f) [%s]\" %(scores.mean(), scores.std(), label))\n",
    "        \n",
    "    clf.fit(X, y)\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n",
    "    plt.title(label)\n",
    "    plt.xlabel('pdays')\n",
    "    plt.ylabel('euribor3m')\n",
    "    plt.savefig(os.path.join('1-4-NN_(pdaysVSeuribor3m).png'), dpi=300, format='png', bbox_inches='tight')\n",
    "\n",
    "\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Produce a table with the true/false positive/negative metrics as well as accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report for Decision Tree:\")\n",
    "print(\"Accuracy is \", accuracy_score(y_test,y_pred_dt)*100)\n",
    "print(confusion_matrix(y_test,y_pred_dt))\n",
    "print('\\n')\n",
    "print(classification_report(y_test,y_pred_dt))\n",
    "\n",
    "print(\"Classification Report for Random Forests:\")\n",
    "print(\"Accuracy is \", accuracy_score(y_test,RFC_pred)*100)\n",
    "print(confusion_matrix(y_test,RFC_pred))\n",
    "print(classification_report(y_test,RFC_pred))\n",
    "\n",
    "\n",
    "print(\"Classification Report for Neural Network:\")\n",
    "print(\"Accuracy is \", accuracy_score(y_true,y_pred)*100)\n",
    "print(confusion_matrix(y_true,y_pred))\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = ('Decision Tree', 'Random Forest', 'Neural Network')\n",
    "ind = ['Accuracies (%)', 'True Positive', 'False Positive', 'False Negative', 'True Negative']\n",
    "accuracies = [accuracy_score(y_test,y_pred_dt)*100,accuracy_score(y_test,RFC_pred)*100,accuracy_score(y_true,y_pred)*100]\n",
    "true_negative = [confusion_matrix(y_test,y_pred_dt)[0][0], confusion_matrix(y_test,RFC_pred)[0][0],confusion_matrix(y_true,y_pred)[0][0]]\n",
    "false_negative = [confusion_matrix(y_test,y_pred_dt)[0][1],confusion_matrix(y_test,RFC_pred)[0][1], confusion_matrix(y_true,y_pred)[0][1]]\n",
    "false_positive = [confusion_matrix(y_test,y_pred_dt)[1][0],confusion_matrix(y_test,RFC_pred)[1][0],confusion_matrix(y_true,y_pred)[1][0]]\n",
    "true_positive = [confusion_matrix(y_test,y_pred_dt)[1][1],confusion_matrix(y_test,RFC_pred)[1][1],confusion_matrix(y_true,y_pred)[1][1]]\n",
    "\n",
    "arr1 = np.array([accuracies, true_positive,false_positive,false_negative,true_negative]) \n",
    "table_1 = pd.DataFrame(arr1, index = ind, columns = algorithms)\n",
    "\n",
    "table_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "plt.figure(figsize=(20, 20))\n",
    "# plot True positives\n",
    "\n",
    "algorithms = ('Decision Tree', 'Random Forest', 'Neural Network')\n",
    "x_axis = np.arange(len(algorithms))\n",
    "dt_trueP = confusion_matrix(y_test,y_pred_dt)[1][1]\n",
    "rf_trueP = confusion_matrix(y_test,RFC_pred)[1][1]\n",
    "nn_trueP = confusion_matrix(y_true,y_pred)[1][1]\n",
    "trueP = [dt_trueP,rf_trueP,nn_trueP]\n",
    " \n",
    "\n",
    "plt.subplot(4, 2, 1) \n",
    "ax = sns.barplot(x_axis,trueP)\n",
    "ax.set(ylabel ='True Postive', title='True postive values for the classificiation algorithms',xticklabels = algorithms)\n",
    "for p in ax.patches:\n",
    "        ax.annotate(\"%.2f\" % p.get_height(), (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "             ha='center', va='center',color = 'white',  xytext=(0, -20), textcoords='offset points') \n",
    "\n",
    "# plot False pOsitives\n",
    "dt_falseP = confusion_matrix(y_test,y_pred_dt)[1][0]\n",
    "rf_falseP = confusion_matrix(y_test,RFC_pred)[1][0]\n",
    "nn_falseP = confusion_matrix(y_true,y_pred)[1][0]\n",
    "falseP = [dt_falseP,rf_falseP,nn_falseP]\n",
    " \n",
    "plt.subplot(4, 2, 2)\n",
    "ax = sns.barplot(x_axis,falseP)\n",
    "ax.set(ylabel ='False Postive', title='False positive values for the classificiation algorithms',xticklabels = algorithms)\n",
    "for p in ax.patches:\n",
    "        ax.annotate(\"%.2f\" % p.get_height(), (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "             ha='center', va='center', color = 'white', xytext=(0, -20), textcoords='offset points') \n",
    "\n",
    "# plot False Negatives\n",
    "dt_falseN = confusion_matrix(y_test,y_pred_dt)[0][1]\n",
    "rf_falseN = confusion_matrix(y_test,RFC_pred)[0][1]\n",
    "nn_falseN = confusion_matrix(y_true,y_pred)[0][1]\n",
    "falseN = [dt_falseN,rf_falseN,nn_falseN]\n",
    " \n",
    "plt.subplot(4, 2, 3)\n",
    "ax = sns.barplot(x_axis,falseN)\n",
    "ax.set(ylabel ='False Negative', title='False negative values for the classificiation algorithms',xticklabels = algorithms)\n",
    "for p in ax.patches:\n",
    "        ax.annotate(\"%.2f\" % p.get_height(), (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "             ha='center', va='center', color = 'white', xytext=(0, -20), textcoords='offset points')\n",
    "\n",
    "\n",
    "# plot True Negatives\n",
    "\n",
    "dt_trueN = confusion_matrix(y_test,y_pred_dt)[0][0]\n",
    "rf_trueN = confusion_matrix(y_test,RFC_pred)[0][0]\n",
    "nn_trueN = confusion_matrix(y_true,y_pred)[0][0]\n",
    "trueN = [dt_trueN,rf_trueN,nn_trueN]\n",
    "\n",
    "plt.subplot(4, 2, 4) \n",
    "ax = sns.barplot(x_axis,trueN)\n",
    "ax.set(ylabel ='True Negative', title='True negative values for the classificiation algorithms',xticklabels = algorithms)\n",
    "for p in ax.patches:\n",
    "        ax.annotate(\"%.2f\" % p.get_height(), (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "             ha='center', va='center', color = 'white', xytext=(0, -20), textcoords='offset points')\n",
    "\n",
    "\n",
    "\n",
    "# plot Accurcacy\n",
    "dt_accuracy = accuracy_score(y_test,y_pred_dt)*100\n",
    "rf_accuracy = accuracy_score(y_test,RFC_pred)*100\n",
    "nn_accuracy = accuracy_score(y_true,y_pred)*100\n",
    "performance = [dt_accuracy,rf_accuracy,nn_accuracy]\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "plt.subplot(4, 2, 5)\n",
    "ax = sns.barplot(x_axis,performance)\n",
    "ax.set(ylabel ='Accuracy', title='Accuracies for the Classification Algorithms',xticklabels = algorithms)\n",
    "for p in ax.patches:\n",
    "        ax.annotate(\"%.2f\" % p.get_height(), (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "             ha='center', va='center',color = 'white', xytext=(0, -20), textcoords='offset points')\n",
    "        \n",
    "plt.savefig(os.path.join('barcharts_algorithms.png'), dpi=300, format='png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Parameter Selection and Classification (for dataset B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data loading and preprocessing using the Z-score normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataB = pd.read_csv('DataDNA.csv', sep = ',', header= None)\n",
    "df2 = pd.DataFrame(dataB)\n",
    "print(df2.shape)\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2.isnull().sum().sum())\n",
    "# df2.iloc[:,:-1].head()\n",
    "print(df2.shape)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[57].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2[57] = df2[57].map({1: 1, -1: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[57].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Z-score normalized values\n",
    "normalized_df = df2.iloc[:,:-1]\n",
    "std_scale = preprocessing.StandardScaler().fit(normalized_df)\n",
    "df_std = std_scale.transform(normalized_df)\n",
    "df_zscore = pd.DataFrame(df_std)\n",
    "df_zscore.columns =  normalized_df.columns\n",
    "df_zscore.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Parameter Selection: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a) K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_zscore.loc[:,]\n",
    "y = df2[57]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.30,random_state=42)\n",
    "\n",
    "# 5-fold cross-validation with k = [1, 3, 5, 7, 31] for KNN (the n_neighbors parameter)\n",
    "k = [1, 3, 5, 7, 31]\n",
    "k_scores = []\n",
    "\n",
    "for i in k:\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    k_scores.append(scores.mean())\n",
    "    print(\"For k= {}, Accuracies: {}\".format(i, scores))\n",
    "    \n",
    "\n",
    "print('\\n')\n",
    "print('Mean of accuracy scores:', k_scores)\n",
    "print('\\n')\n",
    "print('Length of list', len(k_scores))\n",
    "print('Max of list', max(k_scores))\n",
    "\n",
    "plt.plot(k, k_scores)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Cross-validated accuracy')\n",
    "# plt.title('')\n",
    "plt.savefig(os.path.join('2-2-a-kNN.png'), dpi=300, format='png', bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "#ref: https://www.ritchieng.com/machine-learning-cross-validation/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b) SVM (RBF Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC()\n",
    "param_grid = {'C': [0.1, 0.5, 1, 2, 5,10, 20, 50], 'gamma': [0.01, 0.05, 0.1, 0.5, 1, 2, 5, 10]}\n",
    "grid = GridSearchCV(SVC(kernel='rbf'),param_grid,cv=5,refit=True)\n",
    "grid.fit(X_train,y_train)\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM GATHER DATA\n",
    "rbf_svc = SVC(kernel='rbf', gamma=0.01, C=10,probability=True).fit(X_train,y_train)\n",
    "\n",
    "#PREDICT PROBABILITY SCORE = 2D ARRAY FOR EACH PREDICTION\n",
    "predictedprobSVC = rbf_svc.predict_proba(X_test)\n",
    "\n",
    "#GET ROC DATA\n",
    "fpr, tpr, thresholds = roc_curve(y_test, predictedprobSVC[:,1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "#GRAPH DATA\n",
    "plt.figure()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "#plt.xlim([0.0, 1.0]\n",
    "#plt.ylim([0.0, 1.05])\n",
    "plt.title('SVM Classifier ROC')\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='SVM ROC area = %0.2f)' % roc_auc)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(os.path.join('2-2-b-svm.png'), dpi=300, format='png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "#ref :https://medium.com/datadriveninvestor/computing-an-roc-graph-with-python-a3aa20b9a3fb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training classifiers and reporting the results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a) Classifying the test set using k-NN, SVM, Random Forests, and Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_zscore.loc[:,]\n",
    "y = df2[57]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.30,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knn and SVM with chosen parameters from part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "svm = SVC(kernel='rbf', gamma=0.01, C=10,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train, y_train)\n",
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pred = knn.predict(X_test)\n",
    "svm_pred = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report for k-NN:\")\n",
    "print(\"Accuracy is \", accuracy_score(y_test,knn_pred)*100)\n",
    "print(confusion_matrix(y_test,knn_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_test,knn_pred))\n",
    "\n",
    "print(\"Classification Report for SVM:\")\n",
    "print(\"Accuracy is \", accuracy_score(y_test,svm_pred)*100)\n",
    "print(confusion_matrix(y_test,svm_pred))\n",
    "print(classification_report(y_test,svm_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest and Neural Network classifiers with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state=42)\n",
    "nn_mlp = MLPClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.fit(X_train, y_train)\n",
    "nn_mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_pred = rfc.predict(X_test)\n",
    "nn_mlp_pred = nn_mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report for Random Forests: \")\n",
    "print(\"Accuracy is \", accuracy_score(y_test,rfc_pred)*100)\n",
    "print(confusion_matrix(y_test,rfc_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_test,rfc_pred))\n",
    "\n",
    "print(\"Classification Report for Neural Network:\")\n",
    "print(\"Accuracy is \", accuracy_score(y_test,nn_mlp_pred)*100)\n",
    "print(confusion_matrix(y_test,nn_mlp_pred))\n",
    "print(classification_report(y_test,nn_mlp_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b) Exploring parameters for Random Forest and Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, rfc_pred)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [1, 2, 4, 8, 16, 32, 64, 100, 200]\n",
    "train_results = []\n",
    "test_results = []\n",
    "\n",
    "for estimator in n_estimators:\n",
    "   rf = RandomForestClassifier(n_estimators=estimator, n_jobs=-1, random_state=42)\n",
    "   rf.fit(X_train, y_train)\n",
    "   train_pred = rf.predict(X_train)\n",
    "   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n",
    "   roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "   train_results.append(roc_auc)\n",
    "   y_pred = rf.predict(X_test)\n",
    "   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "   roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "   test_results.append(roc_auc)\n",
    "\n",
    "    \n",
    "line1, = plt.plot(n_estimators, train_results, 'b', label=\"Train AUC\")\n",
    "line2, = plt.plot(n_estimators, test_results, 'r', label=\"Test AUC\")\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.ylabel('AUC score')\n",
    "plt.xlabel('n_estimators')\n",
    "plt.title('AUC plot for RF')\n",
    "plt.savefig(os.path.join('2-3-b-rf(n_estimator).png'), dpi=300, format='png', bbox_inches='tight')\n",
    "plt.show()\n",
    "# Source: https://medium.com/all-things-ai/in-depth-parameter-tuning-for-random-forest-d67bb7e920d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depths = np.linspace(1, 32, 32, endpoint=True)\n",
    "train_results = []\n",
    "test_results = []\n",
    "\n",
    "for max_depth in max_depths:\n",
    "   rf = RandomForestClassifier(max_depth=max_depth, n_estimators=25, n_jobs=-1, random_state=42)\n",
    "   rf.fit(X_train, y_train)\n",
    "   train_pred = rf.predict(X_train)\n",
    "   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n",
    "   roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "   train_results.append(roc_auc)\n",
    "   y_pred = rf.predict(X_test)\n",
    "   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "   roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "   test_results.append(roc_auc)\n",
    "    \n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "line1, = plt.plot(max_depths, train_results, 'b', label=\"Train AUC\")\n",
    "line2, = plt.plot(max_depths, test_results, 'r', label=\"Test AUC\")\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.ylabel('AUC score')\n",
    "plt.xlabel('Tree depth')\n",
    "plt.title('AUC plot for RF')\n",
    "plt.savefig(os.path.join('2-3-b-rf(Treedepth).png'), dpi=300, format='png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "#Source: https://medium.com/all-things-ai/in-depth-parameter-tuning-for-random-forest-d67bb7e920d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_tunned = RandomForestClassifier(n_estimators=25,random_state=42, max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_tuned = rfc_tunned.fit(X_train, y_train)\n",
    "rfc_tuned_pred = rfc_tunned.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Results from the default parameters for Random forests:\")\n",
    "print(\"Accuracy is \", accuracy_score(y_test,rfc_pred)*100)\n",
    "print(confusion_matrix(y_test,rfc_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_test,rfc_pred))\n",
    "print('\\n')\n",
    "print(\"Results from the tuned parameters for Random forests:\")\n",
    "print(\"Accuracy is \", accuracy_score(y_test,rfc_tuned_pred)*100)\n",
    "print(confusion_matrix(y_test,rfc_tuned_pred))\n",
    "print(classification_report(y_test,rfc_tuned_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_mlp = MLPClassifier(random_state=42)\n",
    "nn_mlp.fit(X_train, y_train)\n",
    "nn_mlp_pred = nn_mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_space = {\n",
    "    'max_iter': [2000],\n",
    "    'hidden_layer_sizes': [(39,39), (57,57),(57,57,57), (39,39,39)],\n",
    "    'activation': ['tanh', 'relu','logistic'],\n",
    "    'solver': ['sgd', 'adam','lbfgs'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "    'random_state': [42]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_mlp_tunned = GridSearchCV(nn_mlp, parameter_space, n_jobs=-1, cv=5, refit=True)\n",
    "nn_mlp_tunned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameter set\n",
    "print('Best parameters found:\\n', nn_mlp_tunned.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_mlp_tunned = MLPClassifier(activation = 'relu',alpha = 0.05, learning_rate= 'constant', solver = 'lbfgs', random_state=42, hidden_layer_sizes=(57, 57, 57), max_iter=2000)\n",
    "nn_mlp_tunned.fit(X_train, y_train)\n",
    "y_true, y_pred = y_test, nn_mlp_tunned.predict(X_test)\n",
    "\n",
    "print(\"Results from the default parameters for the neural network:\")\n",
    "print(\"Accuracy is \", accuracy_score(y_test,nn_mlp_pred)*100)\n",
    "print(confusion_matrix(y_test,nn_mlp_pred))\n",
    "print(classification_report(y_test,nn_mlp_pred))\n",
    "print(\"\\n\")\n",
    "print(\"Results from the tunned parameters for the neural network:\")\n",
    "print(\"Accuracy is \", accuracy_score(y_true,y_pred)*100)\n",
    "print(confusion_matrix(y_true,y_pred))\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C) Varying the split of the training-test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testSizes = np.arange(0.60, 0.90, 0.015)\n",
    "testSizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "X = df_zscore.loc[:,]\n",
    "y = df2[57]\n",
    "\n",
    "# Creating an array with test size range from 60% to 90%\n",
    "testSizes = np.arange(0.6, 0.90, 0.015)\n",
    "\n",
    "accuracies= []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f_measures = []\n",
    "training_times = []\n",
    "classification_times = []\n",
    "\n",
    "\n",
    "for i in testSizes:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=i,random_state=42)\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    t0=time()\n",
    "    knn.fit(X_train, y_train)\n",
    "    training_times.append(round(time()-t0, 3)) # Training time rounded to 3 decimal in seconds\n",
    "    t1=time()\n",
    "    knn_pred = knn.predict(X_test)\n",
    "    classification_times.append(round(time()-t1, 3)) # Classification time rounded to 3 decimal in seconds\n",
    "    accuracy = accuracy_score(y_test,knn_pred)\n",
    "    precision,recall,fscore,support=score(y_test,knn_pred,average='macro')\n",
    "    precisions.append(precision*100)\n",
    "    recalls.append(recall*100)\n",
    "    f_measures.append(fscore*100)\n",
    "    accuracies.append(accuracy*100)\n",
    "    \n",
    "\n",
    "print(\"Results from k-NN classifier:\")\n",
    "performance = ('Accuracy', 'Precision', 'Recall','F-measure','Training time(s)','Classification time(s)')\n",
    "ind = ['Average', 'Standard deviation']\n",
    "avg = [np.array(accuracies).mean(),np.array(precisions).mean(),np.array(recalls).mean(),np.array(f_measures).mean(),np.array(training_times).mean(),np.array(classification_times).mean()]\n",
    "sd = [np.array(accuracies).std(),np.array(precisions).std(),np.array(recalls).std(),np.array(f_measures).std(),np.array(training_times).std(),np.array(classification_times).std()]\n",
    "\n",
    "array1 = np.array([avg, sd]) \n",
    "table_2 = pd.DataFrame(array1, index = ind, columns = performance)\n",
    "\n",
    "table_2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df_zscore.loc[:,]\n",
    "# y = df2[57]\n",
    "\n",
    "# # Creating an array with test size range from 60% to 90%\n",
    "# testSizes = np.arange(0.6, 0.90, 0.015)\n",
    "# accuracies = []\n",
    "# precisions = []\n",
    "# recalls = []\n",
    "# f_measures = []\n",
    "# training_times = []\n",
    "# classification_times = []\n",
    "\n",
    "# for i in testSizes:\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=i,random_state=42)\n",
    "#     t0=time()\n",
    "#     svm = SVC(kernel='rbf', gamma=0.01, C=10,random_state=42)\n",
    "#     t0=time()\n",
    "#     svm.fit(X_train,y_train)\n",
    "#     training_times.append(round(time()-t0, 3)) # Training time rounded to 3 decimal in seconds\n",
    "#     t1=time()\n",
    "#     svm_pred = svm.predict(X_test)\n",
    "#     classification_times.append(round(time()-t1, 3)\n",
    "#     accuracy = accuracy_score(y_test,svm_pred)\n",
    "#     precision,recall,fscore,support=score(y_test,svm_pred,average='macro')\n",
    "#     precisions.append(precision*100)\n",
    "#     recalls.append(recall*100)\n",
    "#     f_measures.append(fscore*100)\n",
    "#     accuracies.append(accuracy*100)\n",
    "    \n",
    "# performance = ('Accuracy', 'Precision', 'Recall','F-measure','Training time(s)','Classification time(s)')\n",
    "# ind = ['Average', 'Standard deviation']\n",
    "# avg = [np.array(accuracies).mean(),np.array(precisions).mean(),np.array(recalls).mean(),np.array(f_measures).mean(),np.array(training_times).mean(),np.array(classification_times).mean()]\n",
    "# sd = [np.array(accuracies).std(),np.array(precisions).std(),np.array(recalls).std(),np.array(f_measures).std(),np.array(training_times).std(),np.array(classification_times).std()]\n",
    "\n",
    "# array1 = np.array([avg, sd]) \n",
    "# table_2 = pd.DataFrame(array1, index = ind, columns = performance)\n",
    "\n",
    "# table_2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_zscore.loc[:,]\n",
    "y = df2[57]\n",
    "\n",
    "# Creating an array with test size range from 60% to 90%\n",
    "testSizes = np.arange(0.6, 0.90, 0.015)\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f_measures = []\n",
    "training_times = []\n",
    "classification_times = []\n",
    "\n",
    "for i in testSizes:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=i,random_state=42)\n",
    "    t0=time()\n",
    "    svm = SVC(kernel='rbf', gamma=0.01, C=10,random_state=42)\n",
    "    t0=time()\n",
    "    svm.fit(X_train,y_train)\n",
    "    training_times.append(round(time()-t0, 3)) # Training time rounded to 3 decimal in seconds\n",
    "    t1=time()\n",
    "    svm_pred = svm.predict(X_test)\n",
    "    classification_times.append(round(time()-t1, 3)\n",
    "                                \n",
    "\n",
    "    # Results:\n",
    "#     accuracy = accuracy_score(y_test,svm_pred)\n",
    "#     precision,recall,fscore,support=score(y_test,svm_pred,average='macro')\n",
    "#     precisions.append(precision*100)\n",
    "#     recalls.append(recall*100)\n",
    "#     f_measures.append(fscore*100)\n",
    "#     accuracies.append(accuracy*100)\n",
    "    \n",
    "# performance = ('Accuracy', 'Precision', 'Recall','F-measure','Training time(s)','Classification time(s)')\n",
    "# ind = ['Average', 'Standard deviation']\n",
    "# avg = [np.array(accuracies).mean(),np.array(precisions).mean(),np.array(recalls).mean(),np.array(f_measures).mean(),np.array(training_times).mean(),np.array(classification_times).mean()]\n",
    "# sd = [np.array(accuracies).std(),np.array(precisions).std(),np.array(recalls).std(),np.array(f_measures).std(),np.array(training_times).std(),np.array(classification_times).std()]\n",
    "\n",
    "# array1 = np.array([avg, sd]) \n",
    "# table_2 = pd.DataFrame(array1, index = ind, columns = performance)\n",
    "\n",
    "# table_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_zscore.loc[:,]\n",
    "y = df2[57]\n",
    "\n",
    "# Creating an array with test size range from 60% to 90%\n",
    "testSizes = np.arange(0.6, 0.90, 0.015)\n",
    "\n",
    "accuracies= []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f_measures = []\n",
    "training_times = []\n",
    "classification_times = []\n",
    "\n",
    "\n",
    "for i in testSizes:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=i,random_state=42)\n",
    "    rfc_tunned = RandomForestClassifier(random_state=42)\n",
    "    t0=time()\n",
    "    rfc_tunned.fit(X_train, y_train)\n",
    "    training_times.append(round(time()-t0, 3)) # Training time rounded to 3 decimal in seconds\n",
    "    t1=time()\n",
    "    rfc_tuned_pred = rfc_tunned.predict(X_test)\n",
    "    classification_times.append(round(time()-t1, 3)) # Classification time rounded to 3 decimal in seconds\n",
    "    accuracy = accuracy_score(y_test,rfc_tuned_pred)\n",
    "    precision,recall,fscore,support=score(y_test,rfc_tuned_pred,average='macro')\n",
    "    precisions.append(precision*100)\n",
    "    recalls.append(recall*100)\n",
    "    f_measures.append(fscore*100)\n",
    "    accuracies.append(accuracy*100)\n",
    "    \n",
    "\n",
    "print(\"Third classifier: Results from random forests(default parameters)\")\n",
    "performance = ('Accuracy', 'Precision', 'Recall','F-measure','Training time(s)','Classification time(s)')\n",
    "ind = ['Average', 'Standard deviation']\n",
    "avg = [np.array(accuracies).mean(),np.array(precisions).mean(),np.array(recalls).mean(),np.array(f_measures).mean(),np.array(training_times).mean(),np.array(classification_times).mean()]\n",
    "sd = [np.array(accuracies).std(),np.array(precisions).std(),np.array(recalls).std(),np.array(f_measures).std(),np.array(training_times).std(),np.array(classification_times).std()]\n",
    "\n",
    "array1 = np.array([avg, sd]) \n",
    "table_2 = pd.DataFrame(array1, index = ind, columns = performance)\n",
    "\n",
    "table_2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_zscore.loc[:,]\n",
    "y = df2[57]\n",
    "\n",
    "# Creating an array with test size range from 60% to 90%\n",
    "testSizes = np.arange(0.6, 0.90, 0.015)\n",
    "\n",
    "accuracies= []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f_measures = []\n",
    "training_times = []\n",
    "classification_times = []\n",
    "\n",
    "\n",
    "for i in testSizes:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=i,random_state=42)\n",
    "    nn_mlp = MLPClassifier(random_state=42)\n",
    "    t0=time()\n",
    "    nn_mlp.fit(X_train, y_train)\n",
    "    training_times.append(round(time()-t0, 3)) # Training time rounded to 3 decimal in seconds\n",
    "    t1=time()\n",
    "    nn_mlp_pred = nn_mlp.predict(X_test)\n",
    "    classification_times.append(round(time()-t1, 3)) # Classification time rounded to 3 decimal in seconds\n",
    "    accuracy = accuracy_score(y_test,nn_mlp_pred)\n",
    "    precision,recall,fscore,support=score(y_test,nn_mlp_pred,average='macro')\n",
    "    precisions.append(precision*100)\n",
    "    recalls.append(recall*100)\n",
    "    f_measures.append(fscore*100)\n",
    "    accuracies.append(accuracy*100)\n",
    "    \n",
    "\n",
    "print(\"Fourth classifier: Neural Network (default paramter)\")\n",
    "performance = ('Accuracy', 'Precision', 'Recall','F-measure','Training time(s)','Classification time(s)')\n",
    "ind = ['Average', 'Standard deviation']\n",
    "avg = [np.array(accuracies).mean(),np.array(precisions).mean(),np.array(recalls).mean(),np.array(f_measures).mean(),np.array(training_times).mean(),np.array(classification_times).mean()]\n",
    "sd = [np.array(accuracies).std(),np.array(precisions).std(),np.array(recalls).std(),np.array(f_measures).std(),np.array(training_times).std(),np.array(classification_times).std()]\n",
    "\n",
    "array1 = np.array([avg, sd]) \n",
    "table_2 = pd.DataFrame(array1, index = ind, columns = performance)\n",
    "\n",
    "table_2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_zscore.loc[:,]\n",
    "y = df2[57]\n",
    "\n",
    "# Creating an array with test size range from 60% to 90%\n",
    "testSizes = np.arange(0.6, 0.90, 0.015)\n",
    "\n",
    "accuracies= []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f_measures = []\n",
    "training_times = []\n",
    "classification_times = []\n",
    "\n",
    "\n",
    "for i in testSizes:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=i,random_state=42)\n",
    "    rfc_tunned = RandomForestClassifier(n_estimators=25,random_state=42, max_depth=3)\n",
    "    t0=time()\n",
    "    rfc_tunned.fit(X_train, y_train)\n",
    "    training_times.append(round(time()-t0, 3)) # Training time rounded to 3 decimal in seconds\n",
    "    t1=time()\n",
    "    rfc_tuned_pred = rfc_tunned.predict(X_test)\n",
    "    classification_times.append(round(time()-t1, 3)) # Classification time rounded to 3 decimal in seconds\n",
    "    accuracy = accuracy_score(y_test,rfc_tuned_pred)\n",
    "    precision,recall,fscore,support=score(y_test,rfc_tuned_pred,average='macro')\n",
    "    precisions.append(precision*100)\n",
    "    recalls.append(recall*100)\n",
    "    f_measures.append(fscore*100)\n",
    "    accuracies.append(accuracy*100)\n",
    "    \n",
    "\n",
    "print(\"Fifth classifier: Results for random forests(tuned parameters)\")\n",
    "performance = ('Accuracy', 'Precision', 'Recall','F-measure','Training time(s)','Classification time(s)')\n",
    "ind = ['Average', 'Standard deviation']\n",
    "avg = [np.array(accuracies).mean(),np.array(precisions).mean(),np.array(recalls).mean(),np.array(f_measures).mean(),np.array(training_times).mean(),np.array(classification_times).mean()]\n",
    "sd = [np.array(accuracies).std(),np.array(precisions).std(),np.array(recalls).std(),np.array(f_measures).std(),np.array(training_times).std(),np.array(classification_times).std()]\n",
    "\n",
    "array1 = np.array([avg, sd]) \n",
    "table_2 = pd.DataFrame(array1, index = ind, columns = performance)\n",
    "\n",
    "table_2\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_zscore.loc[:,]\n",
    "y = df2[57]\n",
    "\n",
    "# Creating an array with test size range from 60% to 90%\n",
    "testSizes = np.arange(0.6, 0.90, 0.015)\n",
    "\n",
    "accuracies= []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f_measures = []\n",
    "training_times = []\n",
    "classification_times = []\n",
    "\n",
    "\n",
    "for i in testSizes:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=i,random_state=42)\n",
    "    nn_mlp = MLPClassifier(activation = 'relu',alpha = 0.05, learning_rate= 'constant', solver = 'lbfgs', random_state=42, hidden_layer_sizes=(57, 57, 57))\n",
    "    t0=time()\n",
    "    nn_mlp.fit(X_train, y_train)\n",
    "    training_times.append(round(time()-t0, 3)) # Training time rounded to 3 decimal in seconds\n",
    "    t1=time()\n",
    "    nn_mlp_pred = nn_mlp.predict(X_test)\n",
    "    classification_times.append(round(time()-t1, 3)) # Classification time rounded to 3 decimal in seconds\n",
    "    accuracy = accuracy_score(y_test,nn_mlp_pred)\n",
    "    precision,recall,fscore,support=score(y_test,nn_mlp_pred,average='macro')\n",
    "    precisions.append(precision*100)\n",
    "    recalls.append(recall*100)\n",
    "    f_measures.append(fscore*100)\n",
    "    accuracies.append(accuracy*100)\n",
    "    \n",
    "\n",
    "print(\"Sixth classifier: Neural Network (tuned paramter)\")\n",
    "performance = ('Accuracy', 'Precision', 'Recall','F-measure','Training time(s)','Classification time(s)')\n",
    "ind = ['Average', 'Standard deviation']\n",
    "avg = [np.array(accuracies).mean(),np.array(precisions).mean(),np.array(recalls).mean(),np.array(f_measures).mean(),np.array(training_times).mean(),np.array(classification_times).mean()]\n",
    "sd = [np.array(accuracies).std(),np.array(precisions).std(),np.array(recalls).std(),np.array(f_measures).std(),np.array(training_times).std(),np.array(classification_times).std()]\n",
    "\n",
    "array1 = np.array([avg, sd]) \n",
    "table_2 = pd.DataFrame(array1, index = ind, columns = performance)\n",
    "\n",
    "table_2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
