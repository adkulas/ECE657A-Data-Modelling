\documentclass[journal,twoside,web]{ieeecolor}
\usepackage{generic}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\markboth{\journalname, VOL. XX, NO. XX, XXXX 2017}
{Author \MakeLowercase{\textit{et al.}}: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS (February 2017)}
\begin{document}
\title{Santander Customer Transaction Prediction (March 2019)}\\

\author{Adam Kulas, Ammar Ahmed, and Richard Ozara\\
\institute University of Waterloo,ON N2L 3G1, Canada\\akulas@uwaterloo.ca\\
a272ahme@uwaterloo.ca \\\centerline{roozara@uwaterloo.ca}}

\maketitle



\begin{abstract}
This paper lays down the process and detailed approach attempted to solve a binary classification problem presented by Santander bank. Different machine learning algorithms were explored and implemented and comparison was made among them. All the programming was done in python using different libraries and packages. The paper focuses more on the application oriented domain (high level implementation and evaluation of the chosen models) and does not cover in depth explanation of those implemented algorithms. 

\end{abstract}

\begin{IEEEkeywords}
Binary classification, Boosting, Ensembling, Machine learning, Naive Bayes, lgbm, logistic regression, XGBoost

\end{IEEEkeywords}

\section{Introduction}
\label{sec:introduction}
\IEEEPARstart{T}{he} purpose of this project is to solve a problem presented by the company Santander. Santander is a bank that is originating in Spain. Since 2013 they have been serving customers in northeastern North America. Santander mentions in the competition description that their most common problems are binary classification problems. The main goal for the competition is to predict which customers will make a specific transaction in the future, irrespective of money transacted. The problem presented by this challenge is a binary classification problem. The challenge is hosted on Kaggle.com where competitors submit their predicted results online and are evaluated on area under the (ROC) curve between the predicted probability and the observed target. The approach followed in this paper started with data preprocessing and then to data exploration and visualization. After that, a simple random forest model was created to set a baseline performance bench-mark and to help with extracting important features as part of the exploratory data analysis (EDA). Then, implemented supervised machine learning classffication algorithms such as naive bayes and logistic regression that were more specifically tailored to the dataset and problem space. Lastly, a more advanced models were tested such XGBoost and LGBM which required a lot of exploring and tuning their hyperparamters. Also, techniques such ensembling models were tested and all results were compared and evaluated. Throughout the process of testing those models, cross-validation CV was performed and cross-referenced with kaggle leader board scores.

\subsection{Abbreviations and Acronyms}
CV score = Cross Validation score \\

\section{Description of the data}
The dataset used in this project is provided by Santander and can be downloaded from kaggle.com. There are two main files: train.csv which contains the training set and the test.csv which contains the test set. The size of the train.csv is 200k x 201 (40,200,000 data points) and for the tain.csv is 200k x 202 (40,000,000 data points). The train.csv has the attributes: binary target column, string ID\_code column, and the numeric feature variables (var0 till var 199) while the test.csv contains the string ID\_code and the numeric feature variables, so the task is to predict the value of target column in the test set. The datasets are fully anonymized. Test labels (target) is private - predicted results must be submitted online and submissions are evaluated on area under the ROC curve between the predicted probability and the observed target. Lastly, there are three submissions resets every 24hrs.

\section{Survey review on the dataset}


\section{Challenges in the dataset}
Fully anonymized dataset.\\
Imbalance dataset.




\begin{center}
 \begin{tabular}{||c c c c||} 
 \hline
 Target Variable & Percentage \\ [0.5ex] 
 \hline\hline
 0 & 89.5  \\ 
 \hline
 1 & 10.5 \\[1ex]
 \hline
\end{tabular}
\end{center}







\section{Preprocessing}
For the data preprocessing, missing values were checked first and there was no missing values found. However, the proportion of the data imbalance found was significant with 89.95\% (179,902 samples) of the target label in the train.csv were classified as 0 as compared to 10.05\% of the data labeled as 1 (20,098 samples). This means that there are two options that should be considered, either up-sample minority class or down-sample majority class. Most cases, down sampling is the optimal option, however, there are some cases were upsampling gives an optimal results over the downsampling depending on the type of the dataset. Another options (besides up and downsampling) are either changing the perfomance metrics or penalize algorithms (Cost-Sensitive Training). As far as this project is concerned, both upsampling and downsampling were performed to compare the results. Lastly, the data was normalized using z-score normalization. Even though not all models need a normalized data (such as XGBoost and LGBM), normalizing for all models helps in making a fair evaluation and comparison of the performances of those models. 


\section{exploratory data analysis (EDA)}
EDA refers to the critical process of performing initial investigations on data so as to discover patterns,to spot anomalies,to test hypothesis and to check assumptions with the help of summary statistics and graphical representations [reference: https://towardsdatascience.com/exploratory-data-analysis-8fc1cb20fd15].
The first part of the EDA was checking for linear correlation between the features. Those identified relationship should be removed during feature selection as they effect some of the models such linear regression. However, for this specific dataset it was foound that all 200 features had a correlation coefficient value between -0.1 and 0.1, which indicates that there is no linear relationship between those 200 features. The heatmap (figure~\ref{fig:fig1}) below shows the results of correlation in which it can be observed that it is almost zero. 
\begin{figure}[h!]
  \centering
  \includegraphics[width=3.6in]{project/code/heatmap.png}
  \caption{Heatmap for correlation}
  \label{fig:fig1}
\end{figure}

Since there are 200 features, it is a bit difficult to plot histograms and box plots for all those features, however, to make the task easier, PCA was first used reduce the dimensionality. When running the PCA (figure~\ref{fig:fig2} below), it can be observed that almost all of the 200 features are required to explain 95\% of the variance. Examining the PCA plot, it is possible that the provider of the dataset already have done some preprocessing techniques, PCA possibly. 
\begin{figure}[h!]
  \centering
  \includegraphics[width=3.8in]{project/code/1-pcafeatures.png}
  \caption{PCA}
  \label{fig:fig2}
\end{figure}
Next thing was to build a simple random forest model to extract important features and possibly providing more insights to the dataset. Doing this reduce the amount of time examining all the 200 features and only focus on those top 10 features. GridSearch was used to for finding the best parameters for the random forest. Figure~\ref{fig:fig3} below shows the top 10 features returned from the random forest model. 
\begin{figure}[h!]
  \centering
  \includegraphics[width=3.8in]{project/code/important features.png}
  \caption{Important features from random forest model}
  \label{fig:fig3}
\end{figure}
Histograms were plotted for those top features as shown below. The orange distribution is for the varaible in the train dataset and the green one corresponds to the same variable in the test dataset. Comparing those features from the train and the test sets gives an insight about the scoring for test set. More specifically, the test data does not have target so this means that prediction has to be formatted into csv file and submitted online for AUC-ROC score. However, what this distributions shows is that the train and test sets are almost similar in the distribution which means that it is possible to split the train.csv data in test and train and the cross validation (CV) score obtained in the local machine should be approximately similar to the AUC-ROC score obtained when submitted online. This give a way around the limitation of having 3 submissions a day by evaluating the model performance in the local machine using the CV score. 



\section{Used Methods in the Project}
As stated earlier, benchmark models were first created. The first two models considered here were naive bayes and logistic regression. The reason behind the choice is because these two models are simple to implement as they have a minimal number of hyper-parameters tuning. The next two models considered where XGBoost and LGBM. More in depth high level explanation of these models is followed.  

\subsection{Naive Bayes classifier}
It is a probabilistic machine learning model based on the Bayes theorem with an assumption of independence among the features. Main advantage of this model is that it can be easily trained on small datasets. Although the dataset used in this project is somewhat large, it was downsampled (more explanation will be followed in the later section). Also, the data size was reduced further when splitting the downsampled data into train and test. Both of these helped in providing the initial setup of having a good performing naive bayes model. 

\subsection{Logistic Regression}
Logistic regression is a regression analysis that can be used for predictions when the target variable is dichotomous (binary). It is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables [reference: https://www.statisticssolutions.com/what-is-logistic-regression/].

\subsection{XGBOOST}
XGBoost is a library for developing fast and high performance gradient boosting tree models and can be used a standalone predictor. 

\subsection{LGBM}









\subsection{Color Processing/Printing in IEEE Journals}
IEEE Xplore\textregistered\ at no charge, \underline{http://graphicsqc.ieee.org/}, allows authors to 

\section{Conclusion}
A conclusion section is not required. Although a conclusion may review the 
main points of the paper, do not replicate the abstract as the conclusion. A 
conclusion might elaborate on the importance of the work or suggest 
applications and extensions. 

\appendices

Appendixes, if needed, appear before the acknowledgment.

\section*{Acknowledgment}



\section*{References and Footnotes}

\subsection{References}






\begin{thebibliography}{00}

\bibitem{b1} G. O. Young, ``Synthetic structure of industrial plastics,'' in \emph{Plastics,} 2\textsuperscript{nd} ed., vol. 3, J. Peters, Ed. New York, NY, USA: McGraw-Hill, 1964, pp. 15--64.

\bibitem{b2} W.-K. Chen, \emph{Linear Networks and Systems.} Belmont, CA, USA: Wadsworth, 1993, pp. 123--135.

\bibitem{b3} J. U. Duncombe, ``Infrared navigation---Part I: An assessment of feasibility,'' \emph{IEEE Trans. Electron Devices}, vol. ED-11, no. 1, pp. 34--39, Jan. 1959, 10.1109/TED.2016.2628402.

\bibitem{b4} E. P. Wigner, ``Theory of traveling-wave optical laser,'' \emph{Phys. Rev}., vol. 134, pp. A635--A646, Dec. 1965.

\bibitem{b5} E. H. Miller, ``A note on reflector arrays,'' \emph{IEEE Trans. Antennas Propagat}., to be published.

\bibitem{b6} E. E. Reber, R. L. Michell, and C. J. Carter, ``Oxygen absorption in the earth's atmosphere,'' Aerospace Corp., Los Angeles, CA, USA, Tech. Rep. TR-0200 (4230-46)-3, Nov. 1988.

\bibitem{b7} J. H. Davis and J. R. Cogdell, ``Calibration program for the 16-foot antenna,'' Elect. Eng. Res. Lab., Univ. Texas, Austin, TX, USA, Tech. Memo. NGL-006-69-3, Nov. 15, 1987.

\bibitem{b8} \emph{Transmission Systems for Communications}, 3\textsuperscript{rd} ed., Western Electric Co., Winston-Salem, NC, USA, 1985, pp. 44--60.

\bibitem{b9} \emph{Motorola Semiconductor Data Manual}, Motorola Semiconductor Products Inc., Phoenix, AZ, USA, 1989.

\bibitem{b10} G. O. Young, ``Synthetic structure of industrial
plastics,'' in Plastics, vol. 3, Polymers of Hexadromicon, J. Peters,
Ed., 2\textsuperscript{nd} ed. New York, NY, USA: McGraw-Hill, 1964, pp. 15-64.
[Online]. Available:
\underline{http://www.bookref.com}.



\end{thebibliography}






\end{document}
