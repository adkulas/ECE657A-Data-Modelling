\documentclass[journal,twoside,web]{ieeecolor}
\usepackage{generic}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\markboth{\journalname, VOL. XX, NO. XX, XXXX 2017}
{Author \MakeLowercase{\textit{et al.}}: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS (February 2017)}
\begin{document}
\title{Santander Customer Transaction Prediction (March 2019)}\\

\author{Adam Kulas, Ammar Ahmed, and Richard Ozara\\
\institute University of Waterloo,ON N2L 3G1, Canada\\akulas@uwaterloo.ca\\
a272ahme@uwaterloo.ca \\\centerline{roozara@uwaterloo.ca}}

\maketitle



\begin{abstract}
This paper lays down the process and detailed approach attempted to solve a binary classification problem presented by Santander bank. Different machine learning algorithms were explored and implemented and comparison was made among them. All the programming was done in python using different libraries and packages. The paper focuses more on the application oriented domain (high level implementation and evaluation of the chosen models) and does not cover in depth explanation of those implemented algorithms. 

\end{abstract}

\begin{IEEEkeywords}
Binary classification, Ensembling, gradient boosting, Machine learning, Naive Bayes, lgbm, logistic regression, XGBoost

\end{IEEEkeywords}

\section{Introduction}
\label{sec:introduction}
\IEEEPARstart{T}{he} purpose of this project is to solve a problem presented by the company Santander. Santander is a bank that is originating in Spain. Since 2013 they have been serving customers in northeastern North America. Santander mentions in the competition description that their most common problems are binary classification problems. The main goal for the competition is to predict which customers will make a specific transaction in the future, irrespective of money transacted. The problem presented by this challenge is a binary classification problem. The challenge is hosted on Kaggle.com where competitors submit their predicted results online and are evaluated on area under the (ROC) curve between the predicted probability and the observed target. The approach followed in this paper started with data preprocessing and then to data exploration and visualization. After that, a simple random forest model was created to set a baseline performance bench-mark and to help with extracting important features as part of the exploratory data analysis (EDA). Then, implemented supervised machine learning classffication algorithms such as naive bayes and logistic regression that were more specifically tailored to the dataset and problem space. Lastly, a more advanced models were tested such XGBoost and LGBM which required a lot of exploring and tuning their hyperparamters. Also, techniques such ensembling models were tested and all results were compared and evaluated. Throughout the process of testing those models, cross-validation CV was performed and cross-referenced with kaggle leader board scores.

\subsection{Abbreviations and Acronyms}
CV score = Cross Validation score \\

\section{Description of the data}
The dataset used in this project is provided by Santander and can be downloaded from kaggle.com. There are two main files: train.csv which contains the training set and the test.csv which contains the test set. The size of the train.csv is 200k x 201 (40,200,000 data points) and for the tain.csv is 200k x 202 (40,000,000 data points). The train.csv has the attributes: binary target column, string ID\_code column, and the numeric feature variables (var0 till var 199) while the test.csv contains the string ID\_code and the numeric feature variables, so the task is to predict the value of target column in the test set. The datasets are fully anonymized. Test labels (target) is private - predicted results must be submitted online and submissions are evaluated on area under the ROC curve between the predicted probability and the observed target. Lastly, there are three submissions resets every 24hrs.

\section{Survey review on the dataset}

\\

\section{Challenges in the dataset}

There are several challenges in the dataset that need to be overcome before analyzing the dataset. First, the fully anonymized  features is one of the most significant challenges in the current dataset.  Another challenge is the imbalance in the data since one of the class, which is 89.5\% of the data, is larger than the other class of 10.5\%.
Table 1 shows the imbalance in the data.

\begin{center}
 \begin{tabular}{||c c c c||} 
 \hline
 Target Variable & Percentage \\ [0.5ex] 
 \hline\hline
 0 & 89.5  \\ 
 \hline
 1 & 10.5 \\[1ex]
 \hline
\end{tabular}
\end{center}







\section{Preprocessing}
For the data preprocessing, missing values were checked first and there was no missing values found. However, the proportion of the data imbalance found was significant with 89.95\% (179,902 samples) of the target label in the train.csv were classified as 0 as compared to 10.05\% of the data labeled as 1 (20,098 samples). This means that there are two options that should be considered, either up-sample minority class or down-sample majority class. Most cases, down sampling is the optimal option, however, there are some cases were upsampling gives an optimal results over the downsampling depending on the type of the dataset. Another options (besides up and downsampling) are either changing the perfomance metrics or penalize algorithms (Cost-Sensitive Training). As far as this project is concerned, both upsampling and downsampling were performed to compare the results. Lastly, the data was normalized using z-score normalization. Even though not all models need a normalized data (such as XGBoost and LGBM), normalizing for all models helps in making a fair evaluation and comparison of the performances of those models. 


\section{exploratory data analysis (EDA)}
EDA refers to the critical process of performing initial investigations on data so as to discover patterns,to spot anomalies,to test hypothesis and to check assumptions with the help of summary statistics and graphical representations [reference: https://towardsdatascience.com/exploratory-data-analysis-8fc1cb20fd15].
The first part of the EDA was checking for linear correlation between the features. Those identified relationship should be removed during feature selection as they effect some of the models such linear regression. However, for this specific dataset it was foound that all 200 features had a correlation coefficient value between -0.1 and 0.1, which indicates that there is no linear relationship between those 200 features. The heatmap (figure~\ref{fig:fig1}) below shows the results of correlation in which it can be observed that it is almost zero. 
\begin{figure}[h!]
  \centering
  \includegraphics[width=3.6in]{project/code/heatmap.png}
  \caption{Heatmap for correlation}
  \label{fig:fig1}
\end{figure}

Since there are 200 features, it is a bit difficult to plot histograms and box plots for all those features, however, to make the task easier, PCA was first used reduce the dimensionality. When running the PCA (figure~\ref{fig:fig2} below), it can be observed that almost all of the 200 features are required to explain 95\% of the variance. Examining the PCA plot, it is possible that the provider of the dataset already have done some preprocessing techniques, PCA possibly. 
\begin{figure}[h!]
  \centering
  \includegraphics[width=3.6in]{project/code/1-pcafeatures.png}
  \caption{PCA}
  \label{fig:fig2}
\end{figure}
Next thing was to build a simple random forest model to extract important features and possibly providing more insights to the dataset. Doing this reduce the amount of time examining all the 200 features and only focus on those top 10 features. GridSearch was used to for finding the best parameters for the random forest. Figure~\ref{fig:fig3} below shows the top 10 features returned from the random forest model. 
\begin{figure}[h!]
  \centering
  \includegraphics[width=3.6in]{project/code/important features.png}
  \caption{Important features from random forest model}
  \label{fig:fig3}
\end{figure}

Histograms were plotted for those top features as shown in figure~\ref{fig:fig4} below.

\begin{figure}[h!]
  \centering
  \includegraphics[width=3.1in]{"project/code/Histograms dist. of features".png}
  \caption{Histograms for the top 2 features}
  \label{fig:fig4}
\end{figure}


The orange distribution is for the varaible in the train dataset and the green one corresponds to the same variable in the test dataset. Comparing those features from the train and the test sets gives an insight about the scoring for test set. More specifically, the test data does not have target so this means that prediction has to be formatted into csv file and submitted online for AUC-ROC score. However, what this distributions shows is that the train and test sets are almost similar in the distribution which means that it is possible to split the train.csv data in test and train and the cross validation (CV) score obtained in the local machine should be approximately similar to the AUC-ROC score obtained when submitted online. This give a way around the limitation of having 3 submissions a day by evaluating the model performance in the local machine using the CV score. 



\section{Used Methods in the Project}
As stated earlier, benchmark models were first created. The first two models considered here were naive bayes and logistic regression. The reason behind the choice is because these two models are simple to implement as they have a minimal number of hyper-parameters tuning. The next two models considered where XGBoost and LGBM which are tree based learning algorithms. More high level explanation of these models is followed.  

\subsection{Naive Bayes classifier}
It is a probabilistic machine learning model based on the Bayes theorem with an assumption of independence among the features. Main advantage of this model is that it can be easily trained on small datasets. Although the dataset used in this project is somewhat large, it was downsampled (more explanation will be followed in the later section). Also, the data size was reduced further when splitting the downsampled data into train and test. Both of these helped in providing the initial setup of having a good performing naive bayes model. 

\subsection{Logistic Regression}
Logistic regression is a regression analysis that can be used for predictions when the target variable is dichotomous (binary). It is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables [reference: https://www.statisticssolutions.com/what-is-logistic-regression/].

\subsection{Boosting Algorithms}
In order to understand XGBoost and lgbm, the term gradient descent and gradient boosting must be understood first. Starting with gradient descent, it uses a cost function to measure how close the predicted values are to the actual values. It is desirable to have as little difference as possible between the predicted values and the actual values, other words, minimizing the cost function. This is done by associating weights with the trained models, therefore, the better weights associated with the model, the more accurate predicted outcome are and the cost function will be lower as well. Gradient Descent is an iterative optimization algorithm in which these associated weights are learned and then updated to minimize the cost function. \\
On the other hand, gradient boosting combines the principle of gradient descent and boosting to a supervised learning models(ref: https://www.kdnuggets.com/2017/10/xgboost-concise-technical-overview.html). The idea of boosting is that a set of an ensemble of weak learners built where the wrongly predicted records are given a higher weights (boosted) so that the next model will pay more attention to those misclassifed labels and will try to correctly predict them. This technique converts a group of week learners into a single strong learner. Both XGBoost and LGBM are tree based models in which they take gradient boosting into higher level by adding more functionalities to have a greater performance and higher speed. 

\\

\subsubsection{XGBOOST (eXtreme Gradient Boosting)}
XGBoost is a library for developing fast and high performance gradient boosting tree models and can be used a standalone predictor. It is an advanced implementation of the gradient boosting algorithms. The main key features of XGBoost is scalability, which drives fast learning through parallel and distributed computing and offers efficient memory usage (ref: https://www.analyticsvidhya.com/blog/2018/09/an-end-to-end-guide-to-understand-the-math-behind-xgboost/). There are several systems and algorithmic optimizations made on the XGBoost that made the scalability feature possible. This includes: a novel tree learning algorithm is for handling sparse data and a theoretically justified weighted quantile sketch procedure enables handling instance weights in approximate tree learning (reference: ). XGBoost enables parallel and distributed computing which makes learning faster which enables quicker model exploration (reference: ). More importantly, XGBoost take advantage of out-of-core computation and enables data scientists to process hundred millions of examples on a desktop (reference: ). 



\subsubsection{LGBM}





\section{Conclusion}
A conclusion section is not required. Although a conclusion may review the 
main points of the paper, do not replicate the abstract as the conclusion. A 
conclusion might elaborate on the importance of the work or suggest 
applications and extensions. 
IEEE Xplore\textregistered\ at no charge, \underline{http://graphicsqc.ieee.org/}, allows authors to 

\appendices

Appendixes, if needed, appear before the acknowledgment.

\section*{Acknowledgment}



\section*{References and Footnotes}

\subsection{References}

\nocite{*}
\bibliographystyle{IEEEtran}
\bibliography{ref}


% \begin{thebibliography}{00}

% \bibitem{b1} G. O. Young, ``Synthetic structure of industrial plastics,'' in \emph{Plastics,} 2\textsuperscript{nd} ed., vol. 3, J. Peters, Ed. New York, NY, USA: McGraw-Hill, 1964, pp. 15--64.

% \bibitem{b2} W.-K. Chen, \emph{Linear Networks and Systems.} Belmont, CA, USA: Wadsworth, 1993, pp. 123--135.

% \bibitem{b3} J. U. Duncombe, ``Infrared navigation---Part I: An assessment of feasibility,'' \emph{IEEE Trans. Electron Devices}, vol. ED-11, no. 1, pp. 34--39, Jan. 1959, 10.1109/TED.2016.2628402.

% \bibitem{b4} E. P. Wigner, ``Theory of traveling-wave optical laser,'' \emph{Phys. Rev}., vol. 134, pp. A635--A646, Dec. 1965.

% \bibitem{b5} E. H. Miller, ``A note on reflector arrays,'' \emph{IEEE Trans. Antennas Propagat}., to be published.

% \bibitem{b6} E. E. Reber, R. L. Michell, and C. J. Carter, ``Oxygen absorption in the earth's atmosphere,'' Aerospace Corp., Los Angeles, CA, USA, Tech. Rep. TR-0200 (4230-46)-3, Nov. 1988.

% \bibitem{b7} J. H. Davis and J. R. Cogdell, ``Calibration program for the 16-foot antenna,'' Elect. Eng. Res. Lab., Univ. Texas, Austin, TX, USA, Tech. Memo. NGL-006-69-3, Nov. 15, 1987.

% \bibitem{b8} \emph{Transmission Systems for Communications}, 3\textsuperscript{rd} ed., Western Electric Co., Winston-Salem, NC, USA, 1985, pp. 44--60.

% \bibitem{b9} \emph{Motorola Semiconductor Data Manual}, Motorola Semiconductor Products Inc., Phoenix, AZ, USA, 1989.

% \bibitem{b10} G. O. Young, ``Synthetic structure of industrial
% plastics,'' in Plastics, vol. 3, Polymers of Hexadromicon, J. Peters,
% Ed., 2\textsuperscript{nd} ed. New York, NY, USA: McGraw-Hill, 1964, pp. 15-64.
% [Online]. Available:
% \underline{http://www.bookref.com}.


% \end{thebibliography}






\end{document}
